{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate to working directory - local\n",
    "%cd '/home/abaric/TakeLab/projects/Retriever/Sentiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate to working directory - server\n",
    "%cd '/home/abaric/retriever-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER detection\n",
    "______________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/retriever/sample_20220825.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER pipeline set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GPU\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(1))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(1)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(1)/1024**3,1), 'GB')\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tokenizer, model and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERTiÄ‡ tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"classla/bcms-bertic-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"classla/bcms-bertic-ner\").to('cuda')\n",
    "\n",
    "# Set up ner pipeline \n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\", device=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ner(ner_pipeline, df):\n",
    "    # Init df with detected ner\n",
    "    ner_df = pd.DataFrame()\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows()):\n",
    "\n",
    "        ner_result = ner_pipeline(row['title'])\n",
    "\n",
    "        # If detected NER exists\n",
    "        if ner_result:\n",
    "\n",
    "            for ner in ner_result:\n",
    "\n",
    "                ner_df =  ner_df.append({'title_id': idx,\n",
    "                                        'article_id':row['id'],\n",
    "                                        'title': row['title'],\n",
    "                                        'ner': ner['word'],\n",
    "                                        'ner_type': ner['entity_group'],\n",
    "                                        'ner_begin': ner['start'],\n",
    "                                        'ner_end': ner['end']}, \n",
    "                                        ignore_index=True)\n",
    "\n",
    "    # Set id and index type as int\n",
    "    ner_df['title_id'] = ner_df['title_id'].astype(int)\n",
    "    ner_df['article_id'] = ner_df['article_id'].astype(int)\n",
    "    ner_df['ner_begin'] = ner_df['ner_begin'].astype(int)\n",
    "    ner_df['ner_end'] = ner_df['ner_end'].astype(int)\n",
    "\n",
    "    return ner_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = detect_ner(ner_pipeline, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emphasize NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emphasize named entity in article sentence\n",
    "def emphasize_named_entity(text, begin_ner, end_ner, start_string, end_string):\n",
    "    return text[:begin_ner] + start_string + text[begin_ner:end_ner] + end_string + text[end_ner:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df['ner_text'] = ner_df.apply(lambda x: emphasize_named_entity(x['title'], x['ner_begin'], x['ner_end'], '<strong>', '</strong>'), axis=1)\n",
    "ner_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df['document_id'] = ner_df.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = ner_df[['article_id', 'title_id', 'document_id', 'title', 'text', 'ner', 'ner_begin', 'ner_end', 'ner_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard inadequate NERs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = ner_df[~((ner_df['ner'].apply(len) == 1)                                             |    # discard one-charachter NERs\n",
    "                  (ner_df['ner'].isin(['N1', 'narod', 'narod.hr', 'Novi list', 'Dnevno.hr'])) |    # discard portal names as detected NERs \n",
    "                  (ner_df['ner'].apply(len) == ner_df['title'].apply(len))                    |    # discard titles that contain only NER\n",
    "                  ((ner_df['ner'].apply(len) == 2) & (ner_df['ner'].str.islower()))                # discard NER that contain two-charachters and are lowercase\n",
    "                  )]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge ner_df with article metadata from Retriever sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv('data/retriever/sample_20220825.csv')\n",
    "articles_df.rename(columns={'id': 'article_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = pd.merge(ner_df, articles_df[['article_id', 'portal', 'date_published', 'body', 'url']], on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df.to_csv('data/headlines_ner.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ner_df for Alanno\n",
    "This sample will be used for annotation in production rounds in Alanno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ner_df = ner_df.sample(n=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ner_df.to_csv('data/sampled_headlines_ner.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "580ed4c552a31dcc267fc5c50ca762b67fa9da7c5973d623a66aff125be5d655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
